{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "# This model is from https://github.com/patrik-ha/explainable-minichess\n",
    "model = keras.models.load_model(\"6x6_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import backend as K\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "def get_all_resblock_outputs(boards):\n",
    "    \"\"\"Gets outputs of all intermediate layers in all residual blocks\"\"\"\n",
    "    if len(boards.shape) == 3:\n",
    "        boards = np.reshape(boards, (1, *boards.shape))\n",
    "\n",
    "    # All inputs\n",
    "    inp = model.input\n",
    "    # All outputs of the residual blocks\n",
    "    layer_names = [layer.name for layer in model.layers if \"conv\" in layer.name or \"res\" in layer.name]\n",
    "    outputs = [layer.output for layer in model.layers if \"conv\" in layer.name or \"res\" in layer.name]\n",
    "    functor = K.function([inp], outputs)\n",
    "\n",
    "    BATCH_SIZE = 32\n",
    "    all_layer_outs = []\n",
    "    for i in tqdm(range(0, boards.shape[0], BATCH_SIZE)):\n",
    "        layer_outs = functor([boards[i:i + BATCH_SIZE]])\n",
    "        all_layer_outs.append(layer_outs)\n",
    "\n",
    "    return all_layer_outs, layer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_NAMES_TO_TARGET = [\"conv2d_2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, balanced_accuracy_score\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "\n",
    "def perform_regression(points, targets, validation_points, validation_targets):\n",
    "    return perform_logistic_regression(points, targets, validation_points, validation_targets)\n",
    "\n",
    "\n",
    "def perform_logistic_regression(points, targets, validation_points, validation_targets):\n",
    "    print(\"sd\")\n",
    "    inputs = keras.layers.Input((points.shape[1]))\n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=keras.regularizers.L1(0.002))(inputs)\n",
    "\n",
    "    model = keras.Model(inputs, output)\n",
    "    model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=keras.optimizers.Adam(learning_rate=0.0001))\n",
    "\n",
    "    model.fit(points, targets, validation_data=(validation_points, validation_targets), epochs=50)\n",
    "\n",
    "    train_preds = model.predict(points) > 0.5\n",
    "    val_preds = model.predict(validation_points) > 0.5\n",
    "    print(binary_accuracy_metric(targets, train_preds))\n",
    "    return binary_accuracy_metric(validation_targets, val_preds), model\n",
    "\n",
    "\n",
    "def binary_accuracy_metric(targets, predictions):\n",
    "    return 2 * (((targets == np.squeeze(predictions)).sum() / len(targets)) - 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CONCEPT_NAME in [\"threat_my_queen\"]:\n",
    "    data = np.load(\"concept_datasets/{}.npz\".format(CONCEPT_NAME))\n",
    "    all_cases = data[\"cases\"]\n",
    "    all_labels = data[\"labels\"]\n",
    "    import os\n",
    "\n",
    "    POSITIONS_TO_CONSIDER = 40000\n",
    "    VALIDATION_POSITIONS = 10000\n",
    "\n",
    "    USE_CACHED_PROBES = False\n",
    "    SAVE_PROBES_IN_CACHE = True\n",
    "\n",
    "    print(\"Getting outputs...\")\n",
    "    outputs, names = get_all_resblock_outputs(all_cases)\n",
    "    # actual_outputs = predictor_model.predict(boards, id_vector_to_use)\n",
    "    # Outputs blir returnert i batcher, må flette det sammen\n",
    "    print(\"Merging outputs...\")\n",
    "    merged_outputs = []\n",
    "    for output_batch in outputs:\n",
    "        for i, output_layer in enumerate(output_batch):\n",
    "            if len(merged_outputs) <= i:\n",
    "                merged_outputs.append([])\n",
    "            merged_outputs[i].extend(output_layer)\n",
    "\n",
    "    for i, layer_output in enumerate(merged_outputs):\n",
    "        merged_outputs[i] = np.array(merged_outputs[i])\n",
    "    outputs = merged_outputs\n",
    "    print(\"Outputs merged.\")\n",
    "\n",
    "    probes = {}\n",
    "    for name in LAYER_NAMES_TO_TARGET:\n",
    "        probe_path = \"probes/6x6_{}-{}\".format(CONCEPT_NAME, name)\n",
    "        if USE_CACHED_PROBES and os.path.exists(probe_path):\n",
    "            probe = keras.models.load_model(probe_path)\n",
    "        else:\n",
    "            LAYER_NUMBER_TO_TARGET = names.index(name)\n",
    "            output = outputs[LAYER_NUMBER_TO_TARGET]\n",
    "            points = output.reshape((output.shape[0], np.prod(output.shape[1:])))\n",
    "            print(points.shape)\n",
    "            print(\"Performing regression for layer {}\".format(LAYER_NUMBER_TO_TARGET))\n",
    "            score, probe = perform_regression(\n",
    "                points[:POSITIONS_TO_CONSIDER],\n",
    "                all_labels[:POSITIONS_TO_CONSIDER],\n",
    "                points[POSITIONS_TO_CONSIDER:],\n",
    "                all_labels[POSITIONS_TO_CONSIDER:]\n",
    "            )\n",
    "\n",
    "        if SAVE_PROBES_IN_CACHE and not USE_CACHED_PROBES:\n",
    "            probe.save(probe_path)\n",
    "        \n",
    "        probes[name] = probe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model was trained as a simple neural network classifier to classify the legality of a given position.\n",
    "# It is used to provide a gradient signal to dissuade the maximisation process from generating illegal positions.\n",
    "\n",
    "# In practice, the dataset used to train it was obtained by using the 6x6 chess model to generate a large amount\n",
    "# of self-play games, and then randomly \"corrupting\" 50 % of them. This was done in such a way as to guarantee\n",
    "# that the \"corrupted\" games were not legal positions, i.e removing kings from either position, placing both kings\n",
    "# in check, placing pawns on the first and last ranks, while adding and removing a random amount of pieces.\n",
    "\n",
    "# This method is alright, but in the future it might be cooler to do this again \n",
    "# with the intention to instead judge on \"unlikeliness\", meaning that it could be used\n",
    "# to avoid \"unlikely\" (and therefore also illegal (?)) positions.\n",
    "legality_model = keras.models.load_model(\"6x6_legality_checker\", compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Did not end up using this, but could be cool to test out\n",
    "class WeightedActivityByPiece(keras.regularizers.Regularizer):\n",
    "\n",
    "    def __init__(self, l):\n",
    "        self.l = l\n",
    "        self.piece_weights = [1, 3, 3, 5, 9, 1]\n",
    "        self.offset = 6\n",
    "\n",
    "    def __call__(self, w):\n",
    "        return self.l * tf.reduce_sum(\n",
    "            self.piece_weights[0] * (w[:,:,:,0] + w[:,:,:,self.offset]) + \n",
    "            self.piece_weights[1] * (w[:,:,:,1] + w[:,:,:,1 + self.offset]) + \n",
    "            self.piece_weights[2] * (w[:,:,:,2] + w[:,:,:,2 + self.offset]) + \n",
    "            self.piece_weights[3] * (w[:,:,:,3] + w[:,:,:,3 + self.offset]) + \n",
    "            self.piece_weights[4] * (w[:,:,:,4] + w[:,:,:,4 + self.offset]) + \n",
    "            self.piece_weights[5] * (w[:,:,:,5] + w[:,:,:,5 + self.offset])\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import larq\n",
    "input_shape = all_cases.shape[1:3]\n",
    "\n",
    "illegal_mask = np.ones((*input_shape, 12), dtype=np.bool8)\n",
    "offset = 6\n",
    "# No pawns on the first and last rank\n",
    "illegal_mask[0, :, 0] = 0\n",
    "illegal_mask[0, :, 0 + offset] = 0\n",
    "illegal_mask[-1, :, 0] = 0\n",
    "illegal_mask[-1, :, 0 + offset] = 0\n",
    "\n",
    "# Don't add bishops when not used in 6x6\n",
    "illegal_mask[:, :, 2] = 0\n",
    "illegal_mask[:, :, 2 + offset] = 0\n",
    "\n",
    "illegal_mask = tf.convert_to_tensor(illegal_mask, tf.float32)\n",
    "\n",
    "def add_probe_to_model(model, probe_dict):\n",
    "    names = list(probe_dict.keys())\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # This layer + the added one is then the \"maximized\" input wrt. concept\n",
    "    POSITION_PLANES = 12\n",
    "    \n",
    "    # Since the input is layer 1\n",
    "    inp = keras.layers.Input(1)\n",
    "    positive_mask_base = larq.layers.QuantDense(\n",
    "        input_shape[0] * input_shape[1] * POSITION_PLANES, \n",
    "        use_bias=True, \n",
    "        kernel_quantizer=larq.quantizers.SteHeaviside(),\n",
    "        name=\"trainable_pos\",\n",
    "        activation=None,\n",
    "        kernel_initializer=keras.initializers.Zeros(), \n",
    "        bias_initializer=keras.initializers.Zeros(),\n",
    "        kernel_constraint=larq.constraints.WeightClip(0)\n",
    "    )(inp)\n",
    "\n",
    "    negative_mask_base = larq.layers.QuantDense(\n",
    "        input_shape[0] * input_shape[1], \n",
    "        use_bias=True, \n",
    "        kernel_quantizer=larq.quantizers.SteHeaviside(),\n",
    "        name=\"trainable_neg\",\n",
    "        activation=None,\n",
    "        kernel_initializer=keras.initializers.Zeros(), \n",
    "        bias_initializer=keras.initializers.Zeros(),\n",
    "        kernel_constraint=larq.constraints.WeightClip(0)\n",
    "    )(inp)\n",
    "    positive_mask = keras.layers.Reshape((input_shape[0], input_shape[1], POSITION_PLANES))(positive_mask_base)\n",
    "    positive_mask = keras.layers.Lambda(lambda outputs: keras.backend.clip(outputs, 0, 1), activity_regularizer=keras.regularizers.L1(0.005), name=\"positive_mask\")(positive_mask)\n",
    "    negative_mask = keras.layers.Reshape((input_shape[0], input_shape[1]))(negative_mask_base)\n",
    "    negative_mask = keras.layers.Lambda(lambda outputs: keras.backend.clip(outputs, 0, 1), activity_regularizer=keras.regularizers.L1(0.005), name=\"negative_mask\")(negative_mask)\n",
    "\n",
    "\n",
    "    def fancy_add(a):\n",
    "        mask, inputs = a\n",
    "        inputs = inputs[:,:,:,:12]\n",
    "        reduced = tf.reduce_sum(inputs, axis=-1)\n",
    "        legal_mask = tf.expand_dims(reduced, -1)\n",
    "        legal_mask = tf.repeat(legal_mask, 12, -1)\n",
    "        mask -= legal_mask\n",
    "        mask = tf.multiply(illegal_mask, mask)\n",
    "        return keras.backend.clip(mask, 0, 1)\n",
    "    \n",
    "    def input_add(a):\n",
    "        mask, inputs = a\n",
    "\n",
    "        return mask + inputs[:,:,:,:12]\n",
    "    \n",
    "\n",
    "    def input_sub(a):\n",
    "\n",
    "        board, neg_mask = a\n",
    "        neg_mask = tf.expand_dims(neg_mask, -1)\n",
    "        neg_mask = tf.repeat(neg_mask, 12, -1)\n",
    "        return board[:,:,:,:12] - neg_mask\n",
    "    \n",
    "    # Burde fjerne tilfeller der den vil ha flere brikker på samme felt\n",
    "    def mask_clear(board):\n",
    "        non_crowded = tf.cast(tf.reduce_sum(board, axis=-1) <= 1, tf.float32)\n",
    "        legal_mask = tf.expand_dims(non_crowded, -1)\n",
    "        legal_mask = tf.repeat(legal_mask, 12, -1)\n",
    "        return tf.multiply(board, legal_mask)\n",
    "\n",
    "\n",
    "    # Trekk fra negative mask\n",
    "    sub = keras.layers.Lambda(input_sub)((model.input, negative_mask))\n",
    "    sub = keras.layers.Lambda(lambda outputs: keras.backend.clip(outputs, 0, 1), name=\"after_neg_mask\")(sub)\n",
    "\n",
    "    # Tvinge positive mask til å ikke legge til brikker på toppen av felt som er fylt inn (vil den ta bort brikker så kan den gjøre det gjennom negative mask)\n",
    "    positive_mask = keras.layers.Lambda(fancy_add, name=\"fancy_add\")((positive_mask, sub))\n",
    "\n",
    "    # Legge til brikker\n",
    "    sub = keras.layers.Lambda(input_add)((positive_mask, sub))\n",
    "    sub = keras.layers.Lambda(lambda outputs: keras.backend.clip(outputs, 0, 1))(sub)\n",
    "    sub = keras.layers.Lambda(mask_clear, name=\"new_board\")(sub)\n",
    "\n",
    "    # Legge til legality-checker\n",
    "    legality_model.trainable = False\n",
    "    legality_model._name = \"legality_model\"\n",
    "    legality_x = legality_model(sub)\n",
    "    # for layer in legality_model.layers[1:]:\n",
    "    #     layer.trainable = False\n",
    "    #     layer = layer(legality_x)\n",
    "    #     legality_x = layer\n",
    "\n",
    "    def input_meta(a):\n",
    "        return a[:,:,:,12:]\n",
    "\n",
    "    input_clip = keras.layers.Lambda(input_meta)(model.input)\n",
    "    sub = keras.layers.Concatenate(axis=-1)([sub, input_clip])\n",
    "    # sub = keras.layers.Reshape((shape[1], shape[2], 19))(sub)\n",
    "\n",
    "    # some_model.summary()\n",
    "    x = sub\n",
    "\n",
    "    model_outputs = []\n",
    "    last_res = None\n",
    "    for i in range(1, len(model.layers)):\n",
    "        layer = model.layers[i]\n",
    "        if \"add\" in layer.name:\n",
    "            x = model.layers[i]([last_res, x])\n",
    "        else:\n",
    "            x = model.layers[i](x)\n",
    "        if \"res\" in layer.name:\n",
    "            last_res = x\n",
    "        name = x.name.split(\"/\")[0]\n",
    "        if name in names:\n",
    "            layer_index = names.index(name)\n",
    "            model_intermediate = keras.layers.Flatten()(x)\n",
    "            output = keras.layers.Dense(1, activation=\"sigmoid\", name=\"probe_out_{}\".format(name))(model_intermediate)\n",
    "            model_outputs.append(output)\n",
    "        if len(model_outputs) == len(names):\n",
    "            break\n",
    "\n",
    "    full_model = keras.Model([model.inputs, inp], [*model_outputs, legality_x])\n",
    "    for name, probe in probes.items():\n",
    "        concept_output = full_model.get_layer(\"probe_out_{}\".format(name))\n",
    "        concept_output.set_weights(probe.layers[-1].get_weights())\n",
    "        concept_output.trainable = False\n",
    "    # full_model.get_layer(\"padding\").trainable = False\n",
    "    # full_model.summary()\n",
    "\n",
    "\n",
    "\n",
    "    full_model.compile(loss=keras.losses.BinaryCrossentropy(), optimizer=larq.optimizers.Bop(threshold=1e-7, gamma=0.0000001))\n",
    "    return full_model\n",
    "\n",
    "def best_fit(model, cases, target, epochs=50):\n",
    "    if len(cases.shape) == 3:\n",
    "        cases = np.expand_dims(cases, 0)\n",
    "    ones = np.ones(cases.shape[0])\n",
    "    zeros = np.zeros(cases.shape[0])\n",
    "    targets = np.full((len(probes), cases.shape[0]), target)\n",
    "    model.fit([cases, ones], [*targets, zeros], epochs=epochs, batch_size=1)\n",
    "\n",
    "altered_model = add_probe_to_model(model, probes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_LOOKUP = {\n",
    "    'p': 0,\n",
    "    'r': 3,\n",
    "    'n': 1,\n",
    "    'b': 2,\n",
    "    'k': 5,\n",
    "    'q': 4\n",
    "}\n",
    "\n",
    "INVERSE_PIECE_LOOKUP = {v: k for k, v in PIECE_LOOKUP.items()}\n",
    "\n",
    "import chess\n",
    "\n",
    "def visualize_board(output, override_to_play=0):\n",
    "    if output.shape[-1] < 19:\n",
    "        to_play = override_to_play\n",
    "    else:\n",
    "        to_play = int(output[0, 0, -1])\n",
    "    board = chess.Board(None)\n",
    "    for i in range(output.shape[0]):\n",
    "        for j in range(output.shape[1]):\n",
    "            for turn in [0, 1]:\n",
    "                for piece in range(6):\n",
    "                    if output[i, j, 6 * turn + piece]:\n",
    "                        if to_play == 0:\n",
    "                            file = chess.FILE_NAMES[output.shape[1] - j - 1]\n",
    "                            rank = chess.RANK_NAMES[output.shape[0] - i - 1]\n",
    "                            turn = (1 - turn)\n",
    "                        else:\n",
    "                            file = chess.FILE_NAMES[j]\n",
    "                            rank = chess.RANK_NAMES[i]\n",
    "                        board.set_piece_at(chess.parse_square(file + rank), chess.Piece(1 + piece, turn))\n",
    "    return board\n",
    "# visualize_board(to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_consider = all_cases[114]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>. . . . . . . .\n. . . . . . . .\n. K . . N R . .\nR . P . . . . .\nP . p . . P . .\nP p . P . . . .\n. . N k . . . .\nr . . . . . . .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(105, 285)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 240)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 195)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 195)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 150)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 150)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(60, 105)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(195, 105)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(240, 105)\" /></svg>",
      "text/plain": [
       "Board('8/8/1K2NR2/R1P5/P1p2P2/Pp1P4/2Nk4/r7 w - - 0 1')"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_board(to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 2/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 3/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 4/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 5/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 6/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 7/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 8/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 9/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.2450 - probe_out_conv2d_5_loss: 0.0020 - legality_model_loss: 1.5930\n",
      "Epoch 10/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9670 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 11/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9670 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 12/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9670 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 13/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 0.9670 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 14/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9670 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 15/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 16/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 17/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 18/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 19/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 20/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 21/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 22/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 0.9620 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 23/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9570 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 24/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9570 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 25/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.9570 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 26/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 0.9570 - probe_out_conv2d_5_loss: 0.0568 - legality_model_loss: 0.2652\n",
      "Epoch 27/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4151 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 28/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4151 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 29/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4151 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 30/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4151 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 31/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4151 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 32/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 33/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 34/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 35/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 36/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 37/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 38/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 39/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 40/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4101 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 41/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4051 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 42/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4051 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 43/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4051 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 44/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4051 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 45/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4051 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 46/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4001 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 47/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4001 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 48/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4001 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 49/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 50/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 51/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 52/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 53/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 54/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 55/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 56/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 57/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 58/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 59/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 60/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 61/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 62/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.3951 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 63/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 64/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 65/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 66/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 67/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 68/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 69/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 70/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 71/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 72/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3901 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 73/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 74/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 75/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 76/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 77/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 78/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 79/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 80/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 81/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 82/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 83/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 84/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 85/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3851 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 86/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3801 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 87/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 88/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 89/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 90/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 91/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 92/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 93/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 94/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3751 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 95/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 96/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 97/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 98/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 99/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 100/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 101/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 102/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 103/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 104/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 105/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 106/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3701 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 107/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 108/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 109/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 110/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 111/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 112/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3651 - probe_out_conv2d_5_loss: 0.3851 - legality_model_loss: 0.4000\n",
      "Epoch 113/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.5007 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 114/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 115/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 116/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 117/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 118/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 119/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 120/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 121/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 122/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 123/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 124/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 125/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 126/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 127/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 128/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 129/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 130/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4957 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 131/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 132/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 133/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 134/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 135/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 136/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 137/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 138/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 139/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 140/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 141/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 142/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 143/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 144/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 145/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 146/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 147/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 148/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 149/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 150/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 151/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 152/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 153/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 154/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 155/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 156/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 157/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 158/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 159/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 160/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 161/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 162/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 163/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 164/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 165/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 166/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 167/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 168/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 169/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 170/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 171/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 172/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.4907 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 173/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 174/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 175/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 176/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 177/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 178/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 179/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 180/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4857 - probe_out_conv2d_5_loss: 0.5440 - legality_model_loss: 0.3817\n",
      "Epoch 181/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 182/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 183/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 184/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 185/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 186/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 187/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 188/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 189/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 190/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 191/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 192/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 193/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 194/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 195/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 196/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 197/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 198/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.3303 - probe_out_conv2d_5_loss: 0.3664 - legality_model_loss: 0.4089\n",
      "Epoch 199/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8941 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 200/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8891 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 201/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8891 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 202/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8891 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 203/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8891 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 204/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8891 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 205/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8841 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 206/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8841 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 207/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8841 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 208/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 209/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 210/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 211/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 212/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 213/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 214/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 215/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 216/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 217/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 218/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 219/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 220/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 221/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 222/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 223/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 224/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8791 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 225/500\n",
      "1/1 [==============================] - 0s 18ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 226/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 227/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 228/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 229/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 230/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 231/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 232/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 233/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 234/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 235/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 236/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 237/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 238/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 239/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 240/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 241/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 242/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 243/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 244/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 245/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 246/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 247/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 248/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 249/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 250/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 251/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 252/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 253/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 254/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 255/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8741 - probe_out_conv2d_5_loss: 0.9138 - legality_model_loss: 0.4303\n",
      "Epoch 256/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 257/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 258/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 259/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 260/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 261/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 262/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 263/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 264/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 265/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 266/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 267/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 268/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 269/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 270/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 271/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 272/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 273/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 274/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 275/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 276/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 277/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 278/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 279/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 280/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 281/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 282/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 283/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 284/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 285/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 286/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 287/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 288/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 289/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 290/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 291/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 292/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 293/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 294/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 295/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 296/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 297/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 298/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 299/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 300/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 301/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6536 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 302/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6486 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 303/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6486 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 304/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 305/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 306/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 307/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 308/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 309/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6436 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 310/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 311/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 312/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 313/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 314/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 315/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 316/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 317/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 318/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 319/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 320/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 321/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 322/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 323/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 324/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 325/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 326/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 327/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 328/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6386 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 329/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 330/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 331/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 332/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 333/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 334/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 335/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 336/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 337/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 338/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 339/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 340/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 341/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 342/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 343/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 344/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 345/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 346/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 347/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 348/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 349/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6336 - probe_out_conv2d_5_loss: 0.4143 - legality_model_loss: 0.7143\n",
      "Epoch 350/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6518 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 351/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6518 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 352/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 353/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 354/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 355/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 356/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 357/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 358/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 359/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 360/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 361/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 362/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 363/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 364/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 365/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 366/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 367/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 368/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 369/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 370/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 371/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 372/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 373/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 374/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 375/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 376/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 377/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 2.6468 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 378/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6418 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 379/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 2.6418 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 380/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6418 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 381/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 2.6418 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 382/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 2.6418 - probe_out_conv2d_5_loss: 1.2250 - legality_model_loss: 0.9268\n",
      "Epoch 383/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8483 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 384/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8483 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 385/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8483 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 386/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8483 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 387/500\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 388/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 389/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 390/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 391/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 392/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 393/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 394/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 395/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 396/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 397/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8433 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 398/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 399/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 400/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 401/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 402/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 403/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 404/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 405/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 406/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 407/500\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 408/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 409/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 410/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 411/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 412/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 413/500\n",
      "1/1 [==============================] - 0s 37ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 414/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 415/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 416/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 417/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 418/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 419/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 420/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 421/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 422/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 423/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.6119 - probe_out_conv2d_5_loss: 0.0402 - legality_model_loss: 1.0967\n",
      "Epoch 424/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.8333 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 425/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8333 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 426/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.8333 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 427/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8333 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 428/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8283 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 429/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8283 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 430/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8283 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 431/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.8283 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 432/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 13.4286 - probe_out_conv2d_5_loss: 11.8371 - legality_model_loss: 1.1314\n",
      "Epoch 433/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.8183 - probe_out_conv2d_5_loss: 0.3488 - legality_model_loss: 1.0145\n",
      "Epoch 434/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 435/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 436/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 437/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 438/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 439/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 440/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 441/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 442/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 443/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 444/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 445/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 446/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 447/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 448/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 449/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 450/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 451/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 452/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 453/500\n",
      "1/1 [==============================] - 0s 16ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 454/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 455/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 456/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 457/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2490 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 458/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4492 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 459/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4492 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 460/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4492 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 461/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4492 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 462/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.4492 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 463/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 464/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 465/500\n",
      "1/1 [==============================] - 0s 10ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 466/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 467/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 468/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 469/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 470/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 471/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.4442 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 472/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 473/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 474/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 475/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 476/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 477/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.4392 - probe_out_conv2d_5_loss: 6.8134e-10 - legality_model_loss: 1.0042\n",
      "Epoch 478/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 479/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 480/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 481/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 482/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 483/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 484/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 485/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 486/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 487/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2290 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 488/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 489/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 490/500\n",
      "1/1 [==============================] - 0s 17ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 491/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 492/500\n",
      "1/1 [==============================] - 0s 13ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 493/500\n",
      "1/1 [==============================] - 0s 15ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 494/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 495/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 496/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 497/500\n",
      "1/1 [==============================] - 0s 19ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 498/500\n",
      "1/1 [==============================] - 0s 14ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 499/500\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n",
      "Epoch 500/500\n",
      "1/1 [==============================] - 0s 12ms/step - loss: 1.2240 - probe_out_conv2d_5_loss: 0.1101 - legality_model_loss: 0.6890\n"
     ]
    }
   ],
   "source": [
    "# TODO: might be nice with a notion of early stopping, if the criterias for the legality checker and the\n",
    "# concept outputs are met. Until then, just retry this until both losses are low enough.\n",
    "best_fit(altered_model, to_consider, 1, epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, cases):\n",
    "    if len(cases.shape) == 3:\n",
    "        cases = np.expand_dims(cases, 0)\n",
    "    ones = np.ones(cases.shape[0])\n",
    "\n",
    "    return model([cases, ones])\n",
    "# predict(altered_model_after_neg, all_cases[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.89577013]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[0.49790323]], dtype=float32)>]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(altered_model, to_consider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "altered_model_inter = keras.Model(altered_model.inputs, altered_model.get_layer(\"new_board\").output)\n",
    "# altered_model_fancy = keras.Model(altered_model.inputs, altered_model.get_layer(\"fancy_add\").output)\n",
    "altered_model_pos = keras.Model(altered_model.inputs, altered_model.get_layer(\"positive_mask\").output)\n",
    "altered_model_neg = keras.Model(altered_model.inputs, altered_model.get_layer(\"negative_mask\").output)\n",
    "altered_model_after_neg = keras.Model(altered_model.inputs, altered_model.get_layer(\"after_neg_mask\").output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" version=\"1.2\" baseProfile=\"tiny\" viewBox=\"0 0 390 390\" width=\"390\" height=\"390\"><desc><pre>. . . . . . . .\n. . . . . . . .\n. K . . N R . .\nR . P . . P . .\nP . p . . . . .\nP p . P Q . . .\n. . N k . . . .\nr . . . Q . . .</pre></desc><defs><g id=\"white-pawn\" class=\"white pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#fff\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"white-knight\" class=\"white knight\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M 22,10 C 32.5,11 38.5,18 38,39 L 15,39 C 15,30 25,32.5 23,18\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 24,18 C 24.38,20.91 18.45,25.37 16,27 C 13,29 13.18,31.34 11,31 C 9.958,30.06 12.41,27.96 11,28 C 10,28 11.19,29.23 10,30 C 9,30 5.997,31 6,26 C 6,24 12,14 12,14 C 12,14 13.89,12.1 14,10.5 C 13.27,9.506 13.5,8.5 13.5,7.5 C 14.5,6.5 16.5,10 16.5,10 L 18.5,10 C 18.5,10 19.28,8.008 21,7 C 22,7 22,10 22,10\" style=\"fill:#ffffff; stroke:#000000;\" /><path d=\"M 9.5 25.5 A 0.5 0.5 0 1 1 8.5,25.5 A 0.5 0.5 0 1 1 9.5 25.5 z\" style=\"fill:#000000; stroke:#000000;\" /><path d=\"M 15 15.5 A 0.5 1.5 0 1 1 14,15.5 A 0.5 1.5 0 1 1 15 15.5 z\" transform=\"matrix(0.866,0.5,-0.5,0.866,9.693,-5.173)\" style=\"fill:#000000; stroke:#000000;\" /></g><g id=\"white-rook\" class=\"white rook\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12 36v-4h21v4H12zM11 14V9h4v2h5V9h5v2h5V9h4v5\" stroke-linecap=\"butt\" /><path d=\"M34 14l-3 3H14l-3-3\" /><path d=\"M31 17v12.5H14V17\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M31 29.5l1.5 2.5h-20l1.5-2.5\" /><path d=\"M11 14h23\" fill=\"none\" stroke-linejoin=\"miter\" /></g><g id=\"white-queen\" class=\"white queen\" fill=\"#fff\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M8 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM24.5 7.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM41 12a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM16 8.5a2 2 0 1 1-4 0 2 2 0 1 1 4 0zM33 9a2 2 0 1 1-4 0 2 2 0 1 1 4 0z\" /><path d=\"M9 26c8.5-1.5 21-1.5 27 0l2-12-7 11V11l-5.5 13.5-3-15-3 15-5.5-14V25L7 14l2 12zM9 26c0 2 1.5 2 2.5 4 1 1.5 1 1 .5 3.5-1.5 1-1.5 2.5-1.5 2.5-1.5 1.5.5 2.5.5 2.5 6.5 1 16.5 1 23 0 0 0 1.5-1 0-2.5 0 0 .5-1.5-1-2.5-.5-2.5-.5-2 .5-3.5 1-2 2.5-2 2.5-4-8.5-1.5-18.5-1.5-27 0z\" stroke-linecap=\"butt\" /><path d=\"M11.5 30c3.5-1 18.5-1 22 0M12 33.5c6-1 15-1 21 0\" fill=\"none\" /></g><g id=\"white-king\" class=\"white king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#fff\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#fff\" /><path d=\"M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" /></g><g id=\"black-pawn\" class=\"black pawn\"><path d=\"M22.5 9c-2.21 0-4 1.79-4 4 0 .89.29 1.71.78 2.38C17.33 16.5 16 18.59 16 21c0 2.03.94 3.84 2.41 5.03-3 1.06-7.41 5.55-7.41 13.47h23c0-7.92-4.41-12.41-7.41-13.47 1.47-1.19 2.41-3 2.41-5.03 0-2.41-1.33-4.5-3.28-5.62.49-.67.78-1.49.78-2.38 0-2.21-1.79-4-4-4z\" fill=\"#000\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" /></g><g id=\"black-rook\" class=\"black rook\" fill=\"#000\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M9 39h27v-3H9v3zM12.5 32l1.5-2.5h17l1.5 2.5h-20zM12 36v-4h21v4H12z\" stroke-linecap=\"butt\" /><path d=\"M14 29.5v-13h17v13H14z\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M14 16.5L11 14h23l-3 2.5H14zM11 14V9h4v2h5V9h5v2h5V9h4v5H11z\" stroke-linecap=\"butt\" /><path d=\"M12 35.5h21M13 31.5h19M14 29.5h17M14 16.5h17M11 14h23\" fill=\"none\" stroke=\"#fff\" stroke-width=\"1\" stroke-linejoin=\"miter\" /></g><g id=\"black-king\" class=\"black king\" fill=\"none\" fill-rule=\"evenodd\" stroke=\"#000\" stroke-width=\"1.5\" stroke-linecap=\"round\" stroke-linejoin=\"round\"><path d=\"M22.5 11.63V6\" stroke-linejoin=\"miter\" /><path d=\"M22.5 25s4.5-7.5 3-10.5c0 0-1-2.5-3-2.5s-3 2.5-3 2.5c-1.5 3 3 10.5 3 10.5\" fill=\"#000\" stroke-linecap=\"butt\" stroke-linejoin=\"miter\" /><path d=\"M11.5 37c5.5 3.5 15.5 3.5 21 0v-7s9-4.5 6-10.5c-4-6.5-13.5-3.5-16 4V27v-3.5c-3.5-7.5-13-10.5-16-4-3 6 5 10 5 10V37z\" fill=\"#000\" /><path d=\"M20 8h5\" stroke-linejoin=\"miter\" /><path d=\"M32 29.5s8.5-4 6.03-9.65C34.15 14 25 18 22.5 24.5l.01 2.1-.01-2.1C20 18 9.906 14 6.997 19.85c-2.497 5.65 4.853 9 4.853 9M11.5 30c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0m-21 3.5c5.5-3 15.5-3 21 0\" stroke=\"#fff\" /></g></defs><rect x=\"0\" y=\"0\" width=\"390\" height=\"390\" fill=\"#212121\" /><g transform=\"translate(20, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(20, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M23.328 10.016q-1.742 0-2.414.398-.672.398-.672 1.36 0 .765.5 1.218.508.445 1.375.445 1.196 0 1.914-.843.727-.852.727-2.258v-.32zm2.867-.594v4.992h-1.437v-1.328q-.492.797-1.227 1.18-.734.375-1.797.375-1.343 0-2.14-.75-.79-.758-.79-2.024 0-1.476.985-2.226.992-.75 2.953-.75h2.016V8.75q0-.992-.656-1.531-.649-.547-1.829-.547-.75 0-1.46.18-.711.18-1.368.539V6.062q.79-.304 1.532-.453.742-.156 1.445-.156 1.898 0 2.836.984.937.985.937 2.985z\" /></g><g transform=\"translate(65, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(65, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.922 10.047q0-1.586-.656-2.485-.649-.906-1.79-.906-1.14 0-1.796.906-.649.899-.649 2.485 0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.789-.898.656-.906.656-2.492zm-4.89-3.055q.452-.781 1.14-1.156.695-.383 1.656-.383 1.594 0 2.586 1.266 1 1.265 1 3.328 0 2.062-1 3.328-.992 1.266-2.586 1.266-.96 0-1.656-.375-.688-.383-1.14-1.164v1.312h-1.446V2.258h1.445z\" /></g><g transform=\"translate(110, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(110, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.96 6v1.344q-.608-.336-1.226-.5-.609-.172-1.234-.172-1.398 0-2.172.89-.773.883-.773 2.485 0 1.601.773 2.492.774.883 2.172.883.625 0 1.234-.164.618-.172 1.227-.508v1.328q-.602.281-1.25.422-.64.14-1.367.14-1.977 0-3.14-1.242-1.165-1.242-1.165-3.351 0-2.14 1.172-3.367 1.18-1.227 3.227-1.227.664 0 1.296.14.633.134 1.227.407z\" /></g><g transform=\"translate(155, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(155, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 6.992V2.258h1.437v12.156h-1.437v-1.312q-.453.78-1.149 1.164-.687.375-1.656.375-1.586 0-2.586-1.266-.992-1.266-.992-3.328 0-2.063.992-3.328 1-1.266 2.586-1.266.969 0 1.656.383.696.375 1.149 1.156zm-4.899 3.055q0 1.586.649 2.492.656.898 1.797.898 1.14 0 1.796-.898.657-.906.657-2.492 0-1.586-.657-2.485-.656-.906-1.796-.906-1.141 0-1.797.906-.649.899-.649 2.485z\" /></g><g transform=\"translate(200, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(200, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.555 9.68v.703h-6.61q.094 1.484.89 2.265.806.774 2.235.774.828 0 1.602-.203.781-.203 1.547-.61v1.36q-.774.328-1.586.5-.813.172-1.649.172-2.093 0-3.32-1.22-1.219-1.218-1.219-3.296 0-2.148 1.157-3.406 1.164-1.266 3.132-1.266 1.766 0 2.79 1.14 1.03 1.134 1.03 3.087zm-1.438-.422q-.015-1.18-.664-1.883-.64-.703-1.703-.703-1.203 0-1.93.68-.718.68-.828 1.914z\" /></g><g transform=\"translate(245, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(245, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M25.285 2.258v1.195H23.91q-.773 0-1.078.313-.297.312-.297 1.125v.773h2.367v1.117h-2.367v7.633H21.09V6.781h-1.375V5.664h1.375v-.61q0-1.46.68-2.124.68-.672 2.156-.672z\" /></g><g transform=\"translate(290, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(290, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M24.973 9.937q0-1.562-.649-2.421-.64-.86-1.804-.86-1.157 0-1.805.86-.64.859-.64 2.421 0 1.555.64 2.415.648.859 1.805.859 1.164 0 1.804-.86.649-.859.649-2.414zm1.437 3.391q0 2.234-.992 3.32-.992 1.094-3.04 1.094-.757 0-1.429-.117-.672-.11-1.304-.344v-1.398q.632.344 1.25.508.617.164 1.257.164 1.414 0 2.118-.743.703-.734.703-2.226v-.711q-.446.773-1.141 1.156-.695.383-1.664.383-1.61 0-2.594-1.227-.984-1.226-.984-3.25 0-2.03.984-3.257.985-1.227 2.594-1.227.969 0 1.664.383t1.14 1.156V5.664h1.438z\" /></g><g transform=\"translate(335, 0) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(335, 375) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M26.164 9.133v5.281h-1.437V9.18q0-1.243-.485-1.86-.484-.617-1.453-.617-1.164 0-1.836.742-.672.742-.672 2.024v4.945h-1.445V2.258h1.445v4.765q.516-.789 1.211-1.18.703-.39 1.617-.39 1.508 0 2.282.938.773.93.773 2.742z\" /></g><g transform=\"translate(0, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(375, 335) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.754 26.996h2.578v-8.898l-2.805.562v-1.437l2.79-.563h1.578v10.336h2.578v1.328h-6.72z\" /></g><g transform=\"translate(0, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(375, 290) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M8.195 26.996h5.508v1.328H6.297v-1.328q.898-.93 2.445-2.492 1.555-1.57 1.953-2.024.758-.851 1.055-1.437.305-.594.305-1.164 0-.93-.657-1.516-.648-.586-1.695-.586-.742 0-1.57.258-.82.258-1.758.781v-1.593q.953-.383 1.781-.578.828-.196 1.516-.196 1.812 0 2.89.906 1.079.907 1.079 2.422 0 .72-.274 1.368-.265.64-.976 1.515-.196.227-1.243 1.313-1.046 1.078-2.953 3.023z\" /></g><g transform=\"translate(0, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(375, 245) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.434 22.035q1.132.242 1.765 1.008.64.766.64 1.89 0 1.727-1.187 2.672-1.187.946-3.375.946-.734 0-1.515-.149-.774-.14-1.602-.43V26.45q.656.383 1.438.578.78.196 1.632.196 1.485 0 2.258-.586.782-.586.782-1.703 0-1.032-.727-1.61-.719-.586-2.008-.586h-1.36v-1.297h1.423q1.164 0 1.78-.46.618-.47.618-1.344 0-.899-.64-1.375-.633-.485-1.82-.485-.65 0-1.391.141-.743.14-1.633.437V16.95q.898-.25 1.68-.375.788-.125 1.484-.125 1.797 0 2.844.82 1.046.813 1.046 2.204 0 .968-.554 1.64-.555.664-1.578.922z\" /></g><g transform=\"translate(0, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(375, 200) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M11.016 18.035L7.03 24.262h3.985zm-.414-1.375h1.984v7.602h1.664v1.312h-1.664v2.75h-1.57v-2.75H5.75v-1.523z\" /></g><g transform=\"translate(0, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(375, 155) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.719 16.66h6.195v1.328h-4.75v2.86q.344-.118.688-.172.343-.063.687-.063 1.953 0 3.094 1.07 1.14 1.07 1.14 2.899 0 1.883-1.171 2.93-1.172 1.039-3.305 1.039-.735 0-1.5-.125-.758-.125-1.57-.375v-1.586q.703.383 1.453.57.75.188 1.586.188 1.351 0 2.14-.711.79-.711.79-1.93 0-1.219-.79-1.93-.789-.71-2.14-.71-.633 0-1.266.14-.625.14-1.281.438z\" /></g><g transform=\"translate(0, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(375, 110) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10.137 21.863q-1.063 0-1.688.727-.617.726-.617 1.992 0 1.258.617 1.992.625.727 1.688.727 1.062 0 1.68-.727.624-.734.624-1.992 0-1.266-.625-1.992-.617-.727-1.68-.727zm3.133-4.945v1.437q-.594-.28-1.204-.43-.601-.148-1.195-.148-1.562 0-2.39 1.055-.82 1.055-.938 3.188.46-.68 1.156-1.04.696-.367 1.531-.367 1.758 0 2.774 1.07 1.023 1.063 1.023 2.899 0 1.797-1.062 2.883-1.063 1.086-2.828 1.086-2.024 0-3.094-1.547-1.07-1.555-1.07-4.5 0-2.766 1.312-4.406 1.313-1.649 3.524-1.649.593 0 1.195.117.61.118 1.266.352z\" /></g><g transform=\"translate(0, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(375, 65) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M6.25 16.66h7.5v.672L9.516 28.324H7.867l3.985-10.336H6.25z\" /></g><g transform=\"translate(0, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><g transform=\"translate(375, 20) scale(0.75, 0.75)\" fill=\"#e5e5e5\" stroke=\"#e5e5e5\"><path d=\"M10 22.785q-1.125 0-1.773.602-.641.601-.641 1.656t.64 1.656q.649.602 1.774.602t1.773-.602q.649-.61.649-1.656 0-1.055-.649-1.656-.64-.602-1.773-.602zm-1.578-.672q-1.016-.25-1.586-.945-.563-.695-.563-1.695 0-1.399.993-2.211 1-.813 2.734-.813 1.742 0 2.734.813.993.812.993 2.21 0 1-.57 1.696-.563.695-1.571.945 1.14.266 1.773 1.04.641.773.641 1.89 0 1.695-1.04 2.602-1.03.906-2.96.906t-2.969-.906Q6 26.738 6 25.043q0-1.117.64-1.89.641-.774 1.782-1.04zm-.578-2.492q0 .906.562 1.414.57.508 1.594.508 1.016 0 1.586-.508.578-.508.578-1.414 0-.906-.578-1.414-.57-.508-1.586-.508-1.023 0-1.594.508-.562.508-.562 1.414z\" /></g><rect x=\"15\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark a1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"330\" width=\"45\" height=\"45\" class=\"square light b1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark c1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"330\" width=\"45\" height=\"45\" class=\"square light d1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark e1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"330\" width=\"45\" height=\"45\" class=\"square light f1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"330\" width=\"45\" height=\"45\" class=\"square dark g1\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"330\" width=\"45\" height=\"45\" class=\"square light h1\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"285\" width=\"45\" height=\"45\" class=\"square light a2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark b2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"285\" width=\"45\" height=\"45\" class=\"square light c2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark d2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"285\" width=\"45\" height=\"45\" class=\"square light e2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark f2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"285\" width=\"45\" height=\"45\" class=\"square light g2\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"285\" width=\"45\" height=\"45\" class=\"square dark h2\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark a3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"240\" width=\"45\" height=\"45\" class=\"square light b3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark c3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"240\" width=\"45\" height=\"45\" class=\"square light d3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark e3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"240\" width=\"45\" height=\"45\" class=\"square light f3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"240\" width=\"45\" height=\"45\" class=\"square dark g3\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"240\" width=\"45\" height=\"45\" class=\"square light h3\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"195\" width=\"45\" height=\"45\" class=\"square light a4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark b4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"195\" width=\"45\" height=\"45\" class=\"square light c4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark d4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"195\" width=\"45\" height=\"45\" class=\"square light e4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark f4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"195\" width=\"45\" height=\"45\" class=\"square light g4\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"195\" width=\"45\" height=\"45\" class=\"square dark h4\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark a5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"150\" width=\"45\" height=\"45\" class=\"square light b5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark c5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"150\" width=\"45\" height=\"45\" class=\"square light d5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark e5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"150\" width=\"45\" height=\"45\" class=\"square light f5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"150\" width=\"45\" height=\"45\" class=\"square dark g5\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"150\" width=\"45\" height=\"45\" class=\"square light h5\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"105\" width=\"45\" height=\"45\" class=\"square light a6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark b6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"105\" width=\"45\" height=\"45\" class=\"square light c6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark d6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"105\" width=\"45\" height=\"45\" class=\"square light e6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark f6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"105\" width=\"45\" height=\"45\" class=\"square light g6\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"105\" width=\"45\" height=\"45\" class=\"square dark h6\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"15\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark a7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"60\" y=\"60\" width=\"45\" height=\"45\" class=\"square light b7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"105\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark c7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"150\" y=\"60\" width=\"45\" height=\"45\" class=\"square light d7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"195\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark e7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"240\" y=\"60\" width=\"45\" height=\"45\" class=\"square light f7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"285\" y=\"60\" width=\"45\" height=\"45\" class=\"square dark g7\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"330\" y=\"60\" width=\"45\" height=\"45\" class=\"square light h7\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"15\" y=\"15\" width=\"45\" height=\"45\" class=\"square light a8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"60\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark b8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"105\" y=\"15\" width=\"45\" height=\"45\" class=\"square light c8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"150\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark d8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"195\" y=\"15\" width=\"45\" height=\"45\" class=\"square light e8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"240\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark f8\" stroke=\"none\" fill=\"#d18b47\" /><rect x=\"285\" y=\"15\" width=\"45\" height=\"45\" class=\"square light g8\" stroke=\"none\" fill=\"#ffce9e\" /><rect x=\"330\" y=\"15\" width=\"45\" height=\"45\" class=\"square dark h8\" stroke=\"none\" fill=\"#d18b47\" /><use href=\"#black-rook\" xlink:href=\"#black-rook\" transform=\"translate(15, 330)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(195, 330)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(105, 285)\" /><use href=\"#black-king\" xlink:href=\"#black-king\" transform=\"translate(150, 285)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 240)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(60, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(150, 240)\" /><use href=\"#white-queen\" xlink:href=\"#white-queen\" transform=\"translate(195, 240)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(15, 195)\" /><use href=\"#black-pawn\" xlink:href=\"#black-pawn\" transform=\"translate(105, 195)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(15, 150)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(105, 150)\" /><use href=\"#white-pawn\" xlink:href=\"#white-pawn\" transform=\"translate(240, 150)\" /><use href=\"#white-king\" xlink:href=\"#white-king\" transform=\"translate(60, 105)\" /><use href=\"#white-knight\" xlink:href=\"#white-knight\" transform=\"translate(195, 105)\" /><use href=\"#white-rook\" xlink:href=\"#white-rook\" transform=\"translate(240, 105)\" /></svg>",
      "text/plain": [
       "Board('8/8/1K2NR2/R1P2P2/P1p5/Pp1PQ3/2Nk4/r3Q3 w - - 0 1')"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_board(predict(altered_model_inter, to_consider)[0], override_to_play=to_consider[0, 0, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PIECE_LOOKUP = {\n",
    "    'p': 0,\n",
    "    'r': 3,\n",
    "    'n': 1,\n",
    "    'b': 2,\n",
    "    'k': 5,\n",
    "    'q': 4\n",
    "}\n",
    "\n",
    "INVERSE_PIECE_LOOKUP = {v: k for k, v in PIECE_LOOKUP.items()}\n",
    "\n",
    "def any_piece_at(state, i, j):\n",
    "    for turn in [0, 1]:\n",
    "        for ind in range(6):\n",
    "            if state[i, j, turn * 6 + ind]:\n",
    "                return ind, turn\n",
    "    return -1, 0\n",
    "\n",
    "def pseudo_fen(state):\n",
    "    fen_string = \"\"\n",
    "    files = \"abcdefghi\"\n",
    "    ranks = \"12345678\"\n",
    "    for rank in range(state.shape[0]):\n",
    "        empties = 0\n",
    "        for file in range(state.shape[1]):\n",
    "            piece, color = any_piece_at(state, rank, file)\n",
    "            if piece == -1:\n",
    "                empties += 1\n",
    "            else:\n",
    "                if empties > 0:\n",
    "                    fen_string += str(empties)\n",
    "                    empties = 0\n",
    "                piece_char = INVERSE_PIECE_LOOKUP[piece]\n",
    "                if color == 1:\n",
    "                    piece_char = piece_char.upper()\n",
    "                fen_string += piece_char\n",
    "        if empties > 0:\n",
    "            fen_string += str(empties)\n",
    "            empties = 0\n",
    "        if rank != state.shape[0] - 1:\n",
    "            fen_string += '/'\n",
    "\n",
    "    return fen_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r5/2Nk2/Pp1P2/P1p2P/R1P3/1K2NR\n",
      "r3Q1/2Nk2/Pp1PQ1/P1p3/R1P2P/1K2NR\n"
     ]
    }
   ],
   "source": [
    "print(pseudo_fen(to_consider))\n",
    "print(pseudo_fen(predict(altered_model_inter, to_consider)[0]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9d9f6b2873fc084d227b15726712ab1900ec0ba5ea6efe9d96b3a051a1838b5f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "296aa7cfa7482e5f6aad7e2805f8e0ffc5d566567ecda4b4da3f6c963a99a665"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
